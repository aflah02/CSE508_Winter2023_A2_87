{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_path = \"data/preprocessed_data/\"\n",
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_saves(obj, file_name):\n",
    "    with open(\"Saves/\" + file_name, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_saves(file_name):\n",
    "    with open(\"Saves/\" + file_name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(text):\n",
    "    lowercased_text = text.lower()\n",
    "    # Tokenization\n",
    "    tokenized_text = word_tokenize(lowercased_text)\n",
    "    # Remove Stopwords\n",
    "    filtered_text = [word for word in tokenized_text if word not in stopword_list]\n",
    "    # Remove Punctuations\n",
    "    temp = []\n",
    "    for word in filtered_text:\n",
    "        temp_word = word\n",
    "        for punc in string.punctuation:\n",
    "            if punc == \"-\":\n",
    "                continue\n",
    "            temp_word = temp_word.replace(punc, '')\n",
    "        temp.append(temp_word)\n",
    "    filtered_text = temp\n",
    "    # Remove Blank Space Tokens\n",
    "    filtered_text = ' '.join(filtered_text).split()\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    def __init__(self, documents, weighting = \"binary\"):\n",
    "        self.documents = documents\n",
    "        self.weighting = weighting\n",
    "        \n",
    "        self.vocab = None\n",
    "\n",
    "        self.N = len(documents)\n",
    "        self.tf = self.compute_tf()\n",
    "        self.idf = self.compute_idf()\n",
    "        self.tf_idf = self.compute_tf_idf()\n",
    "\n",
    "    def compute_tf(self):\n",
    "        tf = {}\n",
    "        for i, document in enumerate(self.documents):\n",
    "            tf[i] = {}\n",
    "            for term in document.split():\n",
    "                if term not in tf[i]:\n",
    "                    tf[i][term] = 0\n",
    "                tf[i][term] += 1\n",
    "\n",
    "        for i in tf:\n",
    "            for term in tf[i]:\n",
    "                if self.weighting == \"binary\":\n",
    "                    if tf[i][term] > 0:\n",
    "                        tf[i][term] = 1\n",
    "                elif self.weighting == \"raw\":\n",
    "                    tf[i][term] = tf[i][term]\n",
    "                elif self.weighting == \"term_frequency\":\n",
    "                    tf[i][term] = tf[i][term] / sum([tf[i][t] for t in tf[i]])\n",
    "                elif self.weighting == \"log_normalization\":\n",
    "                    tf[i][term] = (1 + np.log(tf[i][term]))\n",
    "                elif self.weighting == \"double_normalization\":\n",
    "                    tf[i][term] = (0.5 + 0.5 * (tf[i][term] / max([tf[i][t] for t in tf[i]])))\n",
    "        return tf\n",
    "\n",
    "    def compute_idf(self):\n",
    "        idf = {}\n",
    "        for document in self.documents:\n",
    "            for term in document.split():\n",
    "                if term not in idf:\n",
    "                    idf[term] = 0\n",
    "                idf[term] += 1\n",
    "        for term in idf:\n",
    "            idf[term] = np.log(self.N / idf[term])\n",
    "        return idf\n",
    "    \n",
    "    def compute_tf_idf(self):\n",
    "        tf_idf = np.zeros((self.N, len(self.idf)))\n",
    "        self.vocab = {}\n",
    "        c = 0\n",
    "        for i, document in enumerate(self.documents):\n",
    "            for j, term in enumerate(set(document.split())):\n",
    "                if term not in self.vocab:\n",
    "                    self.vocab[term] = c\n",
    "                    c+=1\n",
    "                term_loc = self.vocab[term]\n",
    "                tf_idf[i][term_loc] += self.tf[i][term] * self.idf[term]\n",
    "        return tf_idf\n",
    "    \n",
    "    def query_processing(self, query):\n",
    "        query = preprocess_query(query)\n",
    "        query_tf = {}\n",
    "        for term in query:\n",
    "            if term not in query_tf:\n",
    "                query_tf[term] = 0\n",
    "            query_tf[term] += 1\n",
    "        for term in query_tf:\n",
    "            if self.weighting == \"binary\":\n",
    "                if query_tf[term] > 0:\n",
    "                    query_tf[term] = 1\n",
    "            elif self.weighting == \"raw\":\n",
    "                query_tf[term] = query_tf[term]\n",
    "            elif self.weighting == \"term_frequency\":\n",
    "                query_tf[term] = query_tf[term] / sum([query_tf[t] for t in query_tf])\n",
    "            elif self.weighting == \"log_normalization\":\n",
    "                query_tf[term] = (1 + np.log(query_tf[term]))\n",
    "            elif self.weighting == \"double_normalization\":\n",
    "                query_tf[term] = (0.5 + 0.5 * (query_tf[term] / max([query_tf[t] for t in query_tf])))\n",
    "\n",
    "        # create a vector\n",
    "\n",
    "        query_vector = np.zeros(len(self.vocab))\n",
    "        for term in query_tf:\n",
    "            if term in self.vocab:\n",
    "                term_loc = self.vocab[term]\n",
    "                query_vector[term_loc] = query_tf[term] * self.idf[term]\n",
    "\n",
    "        return query_vector\n",
    "    \n",
    "    def get_score(self, query):\n",
    "        query_vector = self.query_processing(query)\n",
    "        scores = np.dot(self.tf_idf, query_vector)\n",
    "        return scores\n",
    "    \n",
    "    def get_top_k(self, query, k):\n",
    "        scores = self.get_score(query)\n",
    "        top_k = np.argsort(scores)[::-1][:k]\n",
    "        return top_k\n",
    "\n",
    "    def get_tf(self):\n",
    "        return self.tf\n",
    "\n",
    "    def get_idf(self):\n",
    "        return self.idf\n",
    "\n",
    "    def get_tf_idf(self):\n",
    "        return self.tf_idf\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting_metrics = [\"binary\", \"raw\", \"term_frequency\", \"log_normalization\", \"double_normalization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(preprocessed_data_path):\n",
    "    filenames.append(filename)\n",
    "    with open(preprocessed_data_path + filename, \"r\") as f:\n",
    "        corpus.append(f.read())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_tfidf = TFIDF(corpus, weighting = \"binary\")\n",
    "binary_tfidf_matrix = binary_tfidf.get_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tfidf = TFIDF(corpus, weighting = \"raw\")\n",
    "raw_tfidf_matrix = raw_tfidf.get_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency_tfidf = TFIDF(corpus, weighting = \"term_frequency\")\n",
    "term_frequency_tfidf_matrix = term_frequency_tfidf.get_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_normalization_tfidf = TFIDF(corpus, weighting = \"log_normalization\")\n",
    "log_normalization_tfidf_matrix = log_normalization_tfidf.get_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_normalization_tfidf = TFIDF(corpus, weighting = \"double_normalization\")\n",
    "double_normalization_tfidf_matrix = double_normalization_tfidf.get_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_saves(binary_tfidf_matrix, \"binary_tfidf_matrix.pkl\")\n",
    "dump_saves(raw_tfidf_matrix, \"raw_tfidf_matrix.pkl\")\n",
    "dump_saves(term_frequency_tfidf_matrix, \"term_frequency_tfidf_matrix.pkl\")\n",
    "dump_saves(log_normalization_tfidf_matrix, \"log_normalization_tfidf_matrix.pkl\")\n",
    "dump_saves(double_normalization_tfidf_matrix, \"double_normalization_tfidf_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = binary_tfidf.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_tfidf_matrix = load_saves(\"data/binary_tfidf_matrix.pkl\")\n",
    "# raw_tfidf_matrix = load_saves(\"data/raw_tfidf_matrix.pkl\")\n",
    "# term_frequency_tfidf_matrix = load_saves(\"data/term_frequency_tfidf_matrix.pkl\")\n",
    "# log_normalization_tfidf_matrix = load_saves(\"data/log_normalization_tfidf_matrix.pkl\")\n",
    "# double_normalization_tfidf_matrix = load_saves(\"data/double_normalization_tfidf_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"turbulent incompressible laminar peripheral jets proximity\"\n",
    "query_2 = \"reynolds number and potential shear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar documents for query 1 using binary weighting:  ['cranfield0086', 'cranfield1223', 'cranfield0354', 'cranfield0650', 'cranfield0792']\n",
      "Scores for the top 5 documents using binary weighting:  [72.3430423  38.02766146 35.81756028 34.31538084 24.41982983]\n",
      "-------------\n",
      "Top 5 most similar documents for query 2 using binary weighting:  ['cranfield1037', 'cranfield1251', 'cranfield0530', 'cranfield0964', 'cranfield1188']\n",
      "Scores for the top 5 documents using binary weighting:  [12.76539403 12.76539403 10.33512113 10.33512113 10.33512113]\n"
     ]
    }
   ],
   "source": [
    "binary_query_1_documents = binary_tfidf.get_top_k(query_1, 5)\n",
    "binary_query_1_score = binary_tfidf.get_score(query_1)\n",
    "binary_query_1_documents_fullname = [filenames[i] for i in binary_query_1_documents]\n",
    "\n",
    "print(\"Top 5 most similar documents for query 1 using binary weighting: \", binary_query_1_documents_fullname)\n",
    "print(\"Scores for the top 5 documents using binary weighting: \", binary_query_1_score[binary_query_1_documents])\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "binary_query_2_documents = binary_tfidf.get_top_k(query_2, 5)\n",
    "binary_query_2_score = binary_tfidf.get_score(query_2)\n",
    "binary_query_2_documents_fullname = [filenames[i] for i in binary_query_2_documents]\n",
    "\n",
    "print(\"Top 5 most similar documents for query 2 using binary weighting: \", binary_query_2_documents_fullname)\n",
    "print(\"Scores for the top 5 documents using binary weighting: \", binary_query_2_score[binary_query_2_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar documents for query 1 using raw weighting:  ['cranfield0086', 'cranfield0997', 'cranfield1223', 'cranfield0696', 'cranfield1164']\n",
      "Scores for the top 5 documents using raw weighting:  [120.26625477  81.64698976  65.24332471  54.43132651  48.83965967]\n",
      "-------------\n",
      "Top 5 most similar documents for query 2 using raw weighting:  ['cranfield1244', 'cranfield0814', 'cranfield0484', 'cranfield1098', 'cranfield1271']\n",
      "Scores for the top 5 documents using raw weighting:  [43.61483799 42.72070283 38.49251145 34.23225452 33.95379324]\n"
     ]
    }
   ],
   "source": [
    "raw_query_documents = raw_tfidf.get_top_k(query_1, 5)\n",
    "raw_query_score = raw_tfidf.get_score(query_1)\n",
    "raw_query_documents_fullname = [filenames[i] for i in raw_query_documents]\n",
    "\n",
    "print(\"Top 5 most similar documents for query 1 using raw weighting: \", raw_query_documents_fullname)\n",
    "print(\"Scores for the top 5 documents using raw weighting: \", raw_query_score[raw_query_documents])\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "raw_query_2_documents = raw_tfidf.get_top_k(query_2, 5)\n",
    "raw_query_2_score = raw_tfidf.get_score(query_2)\n",
    "raw_query_2_documents_fullname = [filenames[i] for i in raw_query_2_documents]\n",
    "\n",
    "print(\"Top 5 most similar documents for query 2 using raw weighting: \", raw_query_2_documents_fullname)\n",
    "print(\"Scores for the top 5 documents using raw weighting: \", raw_query_2_score[raw_query_2_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar documents for query 1 using term frequency weighting:  ['cranfield0243', 'cranfield1380', 'cranfield0086', 'cranfield0354', 'cranfield1223']\n",
      "Scores for the top 5 documents using term frequency weighting:  [1.37284835 0.67580791 0.65471535 0.62071943 0.6171313 ]\n",
      "-------------\n",
      "Top 5 most similar documents for query 2 using term frequency weighting:  ['cranfield0920', 'cranfield0854', 'cranfield1121', 'cranfield0171', 'cranfield1188']\n",
      "Scores for the top 5 documents using term frequency weighting:  [0.70277903 0.55289559 0.55206214 0.51732895 0.50448343]\n"
     ]
    }
   ],
   "source": [
    "term_frequency_query_documents = term_frequency_tfidf.get_top_k(query_1, 5)\n",
    "term_frequency_query_score = term_frequency_tfidf.get_score(query_1)\n",
    "term_frequency_query_documents_fullname = [filenames[i] for i in term_frequency_query_documents]\n",
    "\n",
    "print(\"Top 5 most similar documents for query 1 using term frequency weighting: \", term_frequency_query_documents_fullname)\n",
    "print(\"Scores for the top 5 documents using term frequency weighting: \", term_frequency_query_score[term_frequency_query_documents])\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "term_frequency_query_2_documents = term_frequency_tfidf.get_top_k(query_2, 5)\n",
    "term_frequency_query_2_score = term_frequency_tfidf.get_score(query_2)\n",
    "term_frequency_query_2_documents_fullname = [filenames[i] for i in term_frequency_query_2_documents]\n",
    "\n",
    "print(\"Top 5 most similar documents for query 2 using term frequency weighting: \", term_frequency_query_2_documents_fullname)\n",
    "print(\"Scores for the top 5 documents using term frequency weighting: \", term_frequency_query_2_score[term_frequency_query_2_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar documents for query 1 using log normalization weighting:  ['cranfield0086', 'cranfield1223', 'cranfield1094', 'cranfield1164', 'cranfield0997']\n",
      "Scores for the top 5 documents using log normalization weighting:  [105.56088191  52.97739251  41.34636603  41.34636603  37.9897928 ]\n",
      "-------------\n",
      "Top 5 most similar documents for query 2 using log normalization weighting:  ['cranfield0814', 'cranfield1098', 'cranfield1271', 'cranfield0682', 'cranfield1383']\n",
      "Scores for the top 5 documents using log normalization weighting:  [22.3858168  20.4916743  20.25593634 20.25593634 20.25593634]\n"
     ]
    }
   ],
   "source": [
    "log_normalization_query_documents = log_normalization_tfidf.get_top_k(query_1, 5)\n",
    "log_normalization_query_score = log_normalization_tfidf.get_score(query_1)\n",
    "log_normalization_query_documents_fullname = [filenames[i] for i in log_normalization_query_documents]\n",
    "\n",
    "print(\"Top 5 most similar documents for query 1 using log normalization weighting: \", log_normalization_query_documents_fullname)\n",
    "print(\"Scores for the top 5 documents using log normalization weighting: \", log_normalization_query_score[log_normalization_query_documents])\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "log_normalization_query_2_documents = log_normalization_tfidf.get_top_k(query_2, 5)\n",
    "log_normalization_query_2_score = log_normalization_tfidf.get_score(query_2)\n",
    "log_normalization_query_2_documents_fullname = [filenames[i] for i in log_normalization_query_2_documents]\n",
    "\n",
    "print(\"Top 5 most similar documents for query 2 using log normalization weighting: \", log_normalization_query_2_documents_fullname)\n",
    "print(\"Scores for the top 5 documents using log normalization weighting: \", log_normalization_query_2_score[log_normalization_query_2_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar documents for query 1 using double normalization weighting:  ['cranfield0086', 'cranfield0354', 'cranfield0650', 'cranfield1223', 'cranfield1380']\n",
      "Scores for the top 5 documents using double normalization weighting:  [56.21589695 26.67539778 25.73653563 25.5381632  24.41982983]\n",
      "-------------\n",
      "Top 5 most similar documents for query 2 using double normalization weighting:  ['cranfield1037', 'cranfield0530', 'cranfield1251', 'cranfield1188', 'cranfield0814']\n",
      "Scores for the top 5 documents using double normalization weighting:  [12.76539403 10.33512113 10.10866373  9.90826058  8.62767895]\n"
     ]
    }
   ],
   "source": [
    "double_normalization_query_documents = double_normalization_tfidf.get_top_k(query_1, 5)\n",
    "double_normalization_query_score = double_normalization_tfidf.get_score(query_1)\n",
    "double_normalization_query_documents_fullname = [filenames[i] for i in double_normalization_query_documents]\n",
    "\n",
    "print(\"Top 5 most similar documents for query 1 using double normalization weighting: \", double_normalization_query_documents_fullname)\n",
    "print(\"Scores for the top 5 documents using double normalization weighting: \", double_normalization_query_score[double_normalization_query_documents])\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "double_normalization_query_2_documents = double_normalization_tfidf.get_top_k(query_2, 5)\n",
    "double_normalization_query_2_score = double_normalization_tfidf.get_score(query_2)\n",
    "double_normalization_query_2_documents_fullname = [filenames[i] for i in double_normalization_query_2_documents]\n",
    "\n",
    "print(\"Top 5 most similar documents for query 2 using double normalization weighting: \", double_normalization_query_2_documents_fullname)\n",
    "print(\"Scores for the top 5 documents using double normalization weighting: \", double_normalization_query_2_score[double_normalization_query_2_documents])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jaccard:\n",
    "    def __init__(self, documents, vocab):\n",
    "        self.documents = documents\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def compute_jaccard(self, query):\n",
    "        jaccard_coeff = np.zeros(len(self.documents))\n",
    "        for i, document1 in enumerate(self.documents):\n",
    "            jaccard_coeff[i] = len(set(document1.split()) & set(query.split())) / len(set(document1.split()) | set(query.split()))\n",
    "        return jaccard_coeff\n",
    "\n",
    "    def get_top_k(self, query, k):\n",
    "        jaccard_coeff = self.compute_jaccard(query)\n",
    "        top_k = np.argsort(jaccard_coeff)[::-1][:k]\n",
    "        return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = Jaccard(corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most similar documents for query 1 according to jaccard similarity:  ['cranfield0382', 'cranfield0376', 'cranfield0243', 'cranfield0254', 'cranfield1141', 'cranfield0387', 'cranfield0242', 'cranfield0258', 'cranfield0418', 'cranfield0664']\n",
      "Scores of top 10 most similar documents for query 1 according to jaccard similarity:  [0.08695652 0.08108108 0.07894737 0.06666667 0.05882353 0.05882353\n",
      " 0.05714286 0.05714286 0.05714286 0.05405405]\n"
     ]
    }
   ],
   "source": [
    "jaccard_query_1 = jaccard.compute_jaccard(query_1)\n",
    "top_10_jaccard_query_1 = jaccard.get_top_k(query_1, 10)\n",
    "top_10_jaccard_query_1_fullname = [filenames[i] for i in top_10_jaccard_query_1]\n",
    "\n",
    "print(\"Top 10 most similar documents for query 1 according to jaccard similarity: \", top_10_jaccard_query_1_fullname)\n",
    "print(\"Scores of top 10 most similar documents for query 1 according to jaccard similarity: \", jaccard_query_1[top_10_jaccard_query_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most similar documents for query 2 according to jaccard similarity:  ['cranfield0389', 'cranfield0670', 'cranfield0254', 'cranfield1085', 'cranfield0491', 'cranfield0530', 'cranfield0669', 'cranfield0003', 'cranfield0361', 'cranfield0965']\n",
      "Scores of top 10 most similar documents for query 2 according to jaccard similarity:  [0.07692308 0.07692308 0.06896552 0.06060606 0.06       0.05555556\n",
      " 0.05405405 0.05263158 0.05263158 0.05263158]\n"
     ]
    }
   ],
   "source": [
    "jaccard_query_2 = jaccard.compute_jaccard(query_2)\n",
    "top_10_jaccard_query_2 = jaccard.get_top_k(query_2, 10)\n",
    "top_10_jaccard_query_2_fullname = [filenames[i] for i in top_10_jaccard_query_2]\n",
    "\n",
    "print(\"Top 10 most similar documents for query 2 according to jaccard similarity: \", top_10_jaccard_query_2_fullname)\n",
    "print(\"Scores of top 10 most similar documents for query 2 according to jaccard similarity: \", jaccard_query_2[top_10_jaccard_query_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
