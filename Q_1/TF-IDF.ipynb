{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_path = \"data/preprocessed_data/\"\n",
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_saves(obj, file_name):\n",
    "    with open(\"Saves/\" + file_name, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_saves(file_name):\n",
    "    with open(\"Saves/\" + file_name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(text):\n",
    "    lowercased_text = text.lower()\n",
    "    # Tokenization\n",
    "    tokenized_text = word_tokenize(lowercased_text)\n",
    "    # Remove Stopwords\n",
    "    filtered_text = [word for word in tokenized_text if word not in stopword_list]\n",
    "    # Remove Punctuations\n",
    "    temp = []\n",
    "    for word in filtered_text:\n",
    "        temp_word = word\n",
    "        for punc in string.punctuation:\n",
    "            if punc == \"-\":\n",
    "                continue\n",
    "            temp_word = temp_word.replace(punc, '')\n",
    "        temp.append(temp_word)\n",
    "    filtered_text = temp\n",
    "    # Remove Blank Space Tokens\n",
    "    filtered_text = ' '.join(filtered_text).split()\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    def __init__(self, documents, weighting = \"binary\"):\n",
    "        self.documents = documents\n",
    "        self.weighting = weighting\n",
    "        \n",
    "        self.vocab = None\n",
    "\n",
    "        self.N = len(documents)\n",
    "        self.tf = self.compute_tf()\n",
    "        self.idf = self.compute_idf()\n",
    "        self.tf_idf = self.compute_tf_idf()\n",
    "\n",
    "    def compute_tf(self):\n",
    "        tf = {}\n",
    "        for i, document in enumerate(self.documents):\n",
    "            tf[i] = {}\n",
    "            for term in document.split():\n",
    "                if term not in tf[i]:\n",
    "                    tf[i][term] = 0\n",
    "                tf[i][term] += 1\n",
    "\n",
    "        for i in tf:\n",
    "            for term in tf[i]:\n",
    "                if self.weighting == \"binary\":\n",
    "                    if tf[i][term] > 0:\n",
    "                        tf[i][term] = 1\n",
    "                elif self.weighting == \"raw\":\n",
    "                    tf[i][term] = tf[i][term]\n",
    "                elif self.weighting == \"term_frequency\":\n",
    "                    tf[i][term] = tf[i][term] / sum([tf[i][t] for t in tf[i]])\n",
    "                elif self.weighting == \"log_normalization\":\n",
    "                    tf[i][term] = (1 + np.log(tf[i][term]))\n",
    "                elif self.weighting == \"double_normalization\":\n",
    "                    tf[i][term] = (0.5 + 0.5 * (tf[i][term] / max([tf[i][t] for t in tf[i]])))\n",
    "        return tf\n",
    "\n",
    "    def compute_idf(self):\n",
    "        idf = {}\n",
    "        for document in self.documents:\n",
    "            for term in document.split():\n",
    "                if term not in idf:\n",
    "                    idf[term] = 0\n",
    "                idf[term] += 1\n",
    "        for term in idf:\n",
    "            idf[term] = np.log(self.N / idf[term])\n",
    "        return idf\n",
    "    \n",
    "    def compute_tf_idf(self):\n",
    "        tf_idf = np.zeros((self.N, len(self.idf)))\n",
    "        self.vocab = {}\n",
    "        c = 0\n",
    "        for i, document in enumerate(self.documents):\n",
    "            for j, term in enumerate(set(document.split())):\n",
    "                if term not in self.vocab:\n",
    "                    self.vocab[term] = c\n",
    "                    c+=1\n",
    "                term_loc = self.vocab[term]\n",
    "                tf_idf[i][term_loc] += self.tf[i][term] * self.idf[term]\n",
    "        return tf_idf\n",
    "    \n",
    "    def query_processing(self, query):\n",
    "        query = preprocess_query(query)\n",
    "        query_tf = {}\n",
    "        for term in query:\n",
    "            if term not in query_tf:\n",
    "                query_tf[term] = 0\n",
    "            query_tf[term] += 1\n",
    "        for term in query_tf:\n",
    "            if self.weighting == \"binary\":\n",
    "                if query_tf[term] > 0:\n",
    "                    query_tf[term] = 1\n",
    "            elif self.weighting == \"raw\":\n",
    "                query_tf[term] = query_tf[term]\n",
    "            elif self.weighting == \"term_frequency\":\n",
    "                query_tf[term] = query_tf[term] / sum([query_tf[t] for t in query_tf])\n",
    "            elif self.weighting == \"log_normalization\":\n",
    "                query_tf[term] = (1 + np.log(query_tf[term]))\n",
    "            elif self.weighting == \"double_normalization\":\n",
    "                query_tf[term] = (0.5 + 0.5 * (query_tf[term] / max([query_tf[t] for t in query_tf])))\n",
    "\n",
    "        # create a vector\n",
    "\n",
    "        query_vector = np.zeros(len(self.vocab))\n",
    "        for term in query_tf:\n",
    "            if term in self.vocab:\n",
    "                term_loc = self.vocab[term]\n",
    "                query_vector[term_loc] = query_tf[term] * self.idf[term]\n",
    "\n",
    "        return query_vector\n",
    "    \n",
    "    def get_score(self, query):\n",
    "        query_vector = self.query_processing(query)\n",
    "        scores = np.dot(self.tf_idf, query_vector)\n",
    "        return scores\n",
    "    \n",
    "    def get_top_k(self, query, k):\n",
    "        scores = self.get_score(query)\n",
    "        top_k = np.argsort(scores)[::-1][:k]\n",
    "        return top_k\n",
    "\n",
    "    def get_tf(self):\n",
    "        return self.tf\n",
    "\n",
    "    def get_idf(self):\n",
    "        return self.idf\n",
    "\n",
    "    def get_tf_idf(self):\n",
    "        return self.tf_idf\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting_metrics = [\"binary\", \"raw\", \"term_frequency\", \"log_normalization\", \"double_normalization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for filename in os.listdir(preprocessed_data_path):\n",
    "    with open(preprocessed_data_path + filename, \"r\") as f:\n",
    "        corpus.append(f.read())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_tfidf = TFIDF(corpus, weighting = \"binary\")\n",
    "binary_tfidf_matrix = binary_tfidf.get_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tfidf = TFIDF(corpus, weighting = \"raw\")\n",
    "raw_tfidf_matrix = raw_tfidf.get_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency_tfidf = TFIDF(corpus, weighting = \"term_frequency\")\n",
    "term_frequency_tfidf_matrix = term_frequency_tfidf.get_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_normalization_tfidf = TFIDF(corpus, weighting = \"log_normalization\")\n",
    "log_normalization_tfidf_matrix = log_normalization_tfidf.get_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_normalization_tfidf = TFIDF(corpus, weighting = \"double_normalization\")\n",
    "double_normalization_tfidf_matrix = double_normalization_tfidf.get_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_saves(binary_tfidf_matrix, \"binary_tfidf_matrix.pkl\")\n",
    "dump_saves(raw_tfidf_matrix, \"raw_tfidf_matrix.pkl\")\n",
    "dump_saves(term_frequency_tfidf_matrix, \"term_frequency_tfidf_matrix.pkl\")\n",
    "dump_saves(log_normalization_tfidf_matrix, \"log_normalization_tfidf_matrix.pkl\")\n",
    "dump_saves(double_normalization_tfidf_matrix, \"double_normalization_tfidf_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = binary_tfidf.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_tfidf_matrix = load_saves(\"data/binary_tfidf_matrix.pkl\")\n",
    "# raw_tfidf_matrix = load_saves(\"data/raw_tfidf_matrix.pkl\")\n",
    "# term_frequency_tfidf_matrix = load_saves(\"data/term_frequency_tfidf_matrix.pkl\")\n",
    "# log_normalization_tfidf_matrix = load_saves(\"data/log_normalization_tfidf_matrix.pkl\")\n",
    "# double_normalization_tfidf_matrix = load_saves(\"data/double_normalization_tfidf_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"turbulent incompressible laminar peripheral jets proximity\"\n",
    "query_2 = \"reynolds number and potential shear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar documents for query 1 using binary weighting:  [  85 1222  353  649  791]\n",
      "Scores for the top 5 documents using binary weighting:  [72.3430423  38.02766146 35.81756028 34.31538084 24.41982983]\n",
      "-------------\n",
      "Top 5 most similar documents for query 2 using binary weighting:  [1036 1250  529  963 1187]\n",
      "Scores for the top 5 documents using binary weighting:  [12.76539403 12.76539403 10.33512113 10.33512113 10.33512113]\n"
     ]
    }
   ],
   "source": [
    "binary_query_1_documents = binary_tfidf.get_top_k(query_1, 5)\n",
    "binary_query_1_score = binary_tfidf.get_score(query_1)\n",
    "\n",
    "print(\"Top 5 most similar documents for query 1 using binary weighting: \", binary_query_1_documents)\n",
    "print(\"Scores for the top 5 documents using binary weighting: \", binary_query_1_score[binary_query_1_documents])\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "binary_query_2_documents = binary_tfidf.get_top_k(query_2, 5)\n",
    "binary_query_2_score = binary_tfidf.get_score(query_2)\n",
    "\n",
    "print(\"Top 5 most similar documents for query 2 using binary weighting: \", binary_query_2_documents)\n",
    "print(\"Scores for the top 5 documents using binary weighting: \", binary_query_2_score[binary_query_2_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar documents for query 1 using raw weighting:  [  85  996 1222  695 1163]\n",
      "Scores for the top 5 documents using raw weighting:  [120.26625477  81.64698976  65.24332471  54.43132651  48.83965967]\n",
      "-------------\n",
      "Top 5 most similar documents for query 2 using raw weighting:  [1243  813  483 1097 1270]\n",
      "Scores for the top 5 documents using raw weighting:  [43.61483799 42.72070283 38.49251145 34.23225452 33.95379324]\n"
     ]
    }
   ],
   "source": [
    "raw_query_documents = raw_tfidf.get_top_k(query_1, 5)\n",
    "raw_query_score = raw_tfidf.get_score(query_1)\n",
    "\n",
    "print(\"Top 5 most similar documents for query 1 using raw weighting: \", raw_query_documents)\n",
    "print(\"Scores for the top 5 documents using raw weighting: \", raw_query_score[raw_query_documents])\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "raw_query_2_documents = raw_tfidf.get_top_k(query_2, 5)\n",
    "raw_query_2_score = raw_tfidf.get_score(query_2)\n",
    "\n",
    "print(\"Top 5 most similar documents for query 2 using raw weighting: \", raw_query_2_documents)\n",
    "print(\"Scores for the top 5 documents using raw weighting: \", raw_query_2_score[raw_query_2_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar documents for query 1 using term frequency weighting:  [ 242 1379   85  353 1222]\n",
      "Scores for the top 5 documents using term frequency weighting:  [1.37284835 0.67580791 0.65471535 0.62071943 0.6171313 ]\n",
      "-------------\n",
      "Top 5 most similar documents for query 2 using term frequency weighting:  [ 919  853 1120  170 1187]\n",
      "Scores for the top 5 documents using term frequency weighting:  [0.70277903 0.55289559 0.55206214 0.51732895 0.50448343]\n"
     ]
    }
   ],
   "source": [
    "term_frequency_query_documents = term_frequency_tfidf.get_top_k(query_1, 5)\n",
    "term_frequency_query_score = term_frequency_tfidf.get_score(query_1)\n",
    "\n",
    "print(\"Top 5 most similar documents for query 1 using term frequency weighting: \", term_frequency_query_documents)\n",
    "print(\"Scores for the top 5 documents using term frequency weighting: \", term_frequency_query_score[term_frequency_query_documents])\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "term_frequency_query_2_documents = term_frequency_tfidf.get_top_k(query_2, 5)\n",
    "term_frequency_query_2_score = term_frequency_tfidf.get_score(query_2)\n",
    "\n",
    "print(\"Top 5 most similar documents for query 2 using term frequency weighting: \", term_frequency_query_2_documents)\n",
    "print(\"Scores for the top 5 documents using term frequency weighting: \", term_frequency_query_2_score[term_frequency_query_2_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar documents for query 1 using log normalization weighting:  [  85 1222 1093 1163  996]\n",
      "Scores for the top 5 documents using log normalization weighting:  [105.56088191  52.97739251  41.34636603  41.34636603  37.9897928 ]\n",
      "-------------\n",
      "Top 5 most similar documents for query 2 using log normalization weighting:  [ 813 1097 1270  681 1382]\n",
      "Scores for the top 5 documents using log normalization weighting:  [22.3858168  20.4916743  20.25593634 20.25593634 20.25593634]\n"
     ]
    }
   ],
   "source": [
    "log_normalization_query_documents = log_normalization_tfidf.get_top_k(query_1, 5)\n",
    "log_normalization_query_score = log_normalization_tfidf.get_score(query_1)\n",
    "\n",
    "print(\"Top 5 most similar documents for query 1 using log normalization weighting: \", log_normalization_query_documents)\n",
    "print(\"Scores for the top 5 documents using log normalization weighting: \", log_normalization_query_score[log_normalization_query_documents])\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "log_normalization_query_2_documents = log_normalization_tfidf.get_top_k(query_2, 5)\n",
    "log_normalization_query_2_score = log_normalization_tfidf.get_score(query_2)\n",
    "\n",
    "print(\"Top 5 most similar documents for query 2 using log normalization weighting: \", log_normalization_query_2_documents)\n",
    "print(\"Scores for the top 5 documents using log normalization weighting: \", log_normalization_query_2_score[log_normalization_query_2_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar documents for query 1 using double normalization weighting:  [  85  353  649 1222 1379]\n",
      "Scores for the top 5 documents using double normalization weighting:  [56.21589695 26.67539778 25.73653563 25.5381632  24.41982983]\n",
      "-------------\n",
      "Top 5 most similar documents for query 2 using double normalization weighting:  [1036  529 1250 1187  813]\n",
      "Scores for the top 5 documents using double normalization weighting:  [12.76539403 10.33512113 10.10866373  9.90826058  8.62767895]\n"
     ]
    }
   ],
   "source": [
    "double_normalization_query_documents = double_normalization_tfidf.get_top_k(query_1, 5)\n",
    "double_normalization_query_score = double_normalization_tfidf.get_score(query_1)\n",
    "\n",
    "print(\"Top 5 most similar documents for query 1 using double normalization weighting: \", double_normalization_query_documents)\n",
    "print(\"Scores for the top 5 documents using double normalization weighting: \", double_normalization_query_score[double_normalization_query_documents])\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "double_normalization_query_2_documents = double_normalization_tfidf.get_top_k(query_2, 5)\n",
    "double_normalization_query_2_score = double_normalization_tfidf.get_score(query_2)\n",
    "\n",
    "print(\"Top 5 most similar documents for query 2 using double normalization weighting: \", double_normalization_query_2_documents)\n",
    "print(\"Scores for the top 5 documents using double normalization weighting: \", double_normalization_query_2_score[double_normalization_query_2_documents])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jaccard:\n",
    "    def __init__(self, documents, vocab):\n",
    "        self.documents = documents\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def compute_jaccard(self, query):\n",
    "        jaccard_coeff = np.zeros(len(self.documents))\n",
    "        for i, document1 in enumerate(self.documents):\n",
    "            jaccard_coeff[i] = len(set(document1.split()) & set(query.split())) / len(set(document1.split()) | set(query.split()))\n",
    "        return jaccard_coeff\n",
    "\n",
    "    def get_top_k(self, query, k):\n",
    "        jaccard_coeff = self.compute_jaccard(query)\n",
    "        top_k = np.argsort(jaccard_coeff)[::-1][:k]\n",
    "        return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = Jaccard(corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_coeff = jaccard.compute_jaccard(query_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most similar documents according to jaccard similarity:  [381   2 375 179 241 325   3 253  20 240]\n",
      "Scores of top 10 most similar documents according to jaccard similarity:  [0.23809524 0.22222222 0.17142857 0.15384615 0.15151515 0.14814815\n",
      " 0.14285714 0.13793103 0.13333333 0.12820513]\n"
     ]
    }
   ],
   "source": [
    "jaccard_query_1 = jaccard_coeff\n",
    "\n",
    "top_10_jaccard_query_1 = jaccard.get_top_k(query_1, 10)\n",
    "\n",
    "print(\"Top 10 most similar documents according to jaccard similarity: \", top_10_jaccard_query_1)\n",
    "print(\"Scores of top 10 most similar documents according to jaccard similarity: \", jaccard_query_1[top_10_jaccard_query_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
