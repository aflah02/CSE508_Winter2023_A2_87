{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data with column names category, text\n",
    "df = pd.read_csv(r'Data\\BBC News Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
       "1        154  german business confidence slides german busin...  business\n",
       "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
       "4        917  enron bosses in $168m payout eighteen former e...  business"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>double eviction from big brother model caprice...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>dj double act revamp chart show dj duo jk and ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>weak dollar hits reuters revenues at media gro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>apple ipod family expands market apple has exp...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>santy worm makes unwelcome visit thousands of ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1490 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text       Category\n",
       "0     worldcom ex-boss launches defence lawyers defe...       business\n",
       "1     german business confidence slides german busin...       business\n",
       "2     bbc poll indicates economic gloom citizens in ...       business\n",
       "3     lifestyle  governs mobile choice  faster  bett...           tech\n",
       "4     enron bosses in $168m payout eighteen former e...       business\n",
       "...                                                 ...            ...\n",
       "1485  double eviction from big brother model caprice...  entertainment\n",
       "1486  dj double act revamp chart show dj duo jk and ...  entertainment\n",
       "1487  weak dollar hits reuters revenues at media gro...       business\n",
       "1488  apple ipod family expands market apple has exp...           tech\n",
       "1489  santy worm makes unwelcome visit thousands of ...           tech\n",
       "\n",
       "[1490 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop ArticleId column\n",
    "df.drop('ArticleId', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords = set(stopwords)\n",
    " \n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    This function removes punctuation from a string\n",
    "    \"\"\"\n",
    "    import string\n",
    "    new_string = \"\"\n",
    "    for char in text:\n",
    "        if char not in string.punctuation:\n",
    "            new_string += char\n",
    "    return new_string\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    This function removes stopwords from a string\n",
    "    \"\"\"\n",
    "    new_string = \"\"\n",
    "    for word in text.split():\n",
    "        if word not in stopwords:\n",
    "            new_string += word + \" \"\n",
    "    return new_string\n",
    "\n",
    "def lower_case(text):\n",
    "    \"\"\"\n",
    "    This function converts all characters to lower case\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function splits a string into a list of words\n",
    "    \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "    This function lemmatizes a list of words\n",
    "    \"\"\"\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_string = []\n",
    "    for word in text:\n",
    "        new_string.append(lemmatizer.lemmatize(word))\n",
    "    return new_string\n",
    "\n",
    "def stem(text):\n",
    "    \"\"\"\n",
    "    This function stems a list of words\n",
    "    \"\"\"\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    new_string = []\n",
    "    for word in text:\n",
    "        new_string.append(ps.stem(word))\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(remove_punctuation) # remove punctuation\n",
    "df['Text'] = df['Text'].apply(lower_case) # convert to lower case\n",
    "df['Text'] = df['Text'].apply(remove_stopwords) # remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(tokenize) # tokenize\n",
    "df['Text'] = df['Text'].apply(lemmatize) # lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[worldcom, exboss, launch, defence, lawyer, de...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[german, business, confidence, slide, german, ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bbc, poll, indicates, economic, gloom, citize...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[lifestyle, governs, mobile, choice, faster, b...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[enron, boss, 168m, payout, eighteen, former, ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>[double, eviction, big, brother, model, capric...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>[dj, double, act, revamp, chart, show, dj, duo...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>[weak, dollar, hit, reuters, revenue, medium, ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>[apple, ipod, family, expands, market, apple, ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>[santy, worm, make, unwelcome, visit, thousand...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1490 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text       Category\n",
       "0     [worldcom, exboss, launch, defence, lawyer, de...       business\n",
       "1     [german, business, confidence, slide, german, ...       business\n",
       "2     [bbc, poll, indicates, economic, gloom, citize...       business\n",
       "3     [lifestyle, governs, mobile, choice, faster, b...           tech\n",
       "4     [enron, boss, 168m, payout, eighteen, former, ...       business\n",
       "...                                                 ...            ...\n",
       "1485  [double, eviction, big, brother, model, capric...  entertainment\n",
       "1486  [dj, double, act, revamp, chart, show, dj, duo...  entertainment\n",
       "1487  [weak, dollar, hit, reuters, revenue, medium, ...       business\n",
       "1488  [apple, ipod, family, expands, market, apple, ...           tech\n",
       "1489  [santy, worm, make, unwelcome, visit, thousand...           tech\n",
       "\n",
       "[1490 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_ICF:\n",
    "    def __init__(self, classes, ls_text, ls_classes):\n",
    "        \"\"\"\n",
    "        Class to calculate TF, CF, ICF and TF_ICF for a given dataset\n",
    "        \"\"\"\n",
    "        self.classes = classes\n",
    "        self.ls_text = ls_text\n",
    "        self.ls_classes = ls_classes\n",
    "        self.tf = {}\n",
    "        self.cf = {}\n",
    "        self.icf = {}\n",
    "        self.tf_icf = {}\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Driver function to build TF, CF, ICF and TF_ICF\n",
    "        \"\"\"\n",
    "        for class_ in self.classes:\n",
    "            self.tf[class_] = {}\n",
    "            self.build_tf(class_)\n",
    "        self.build_cf()\n",
    "        self.build_icf()\n",
    "        self.build_tf_icf()\n",
    "    \n",
    "    def build_tf(self, class_):\n",
    "        \"\"\"\n",
    "        Method to build TF for a given class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        class_ : str\n",
    "            class for which TF is to be calculated\n",
    "\n",
    "        Performs\n",
    "        --------\n",
    "        1. Filters the text for a given class\n",
    "        2. For each text, calculates the frequency of each word\n",
    "        3. Stores the frequency in self.tf\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        filtered_text = [self.ls_text[i] for i in range(len(self.ls_text)) if self.ls_classes[i] == class_]\n",
    "        for text in filtered_text:\n",
    "            for word in text:\n",
    "                if word not in self.tf[class_]:\n",
    "                    self.tf[class_][word] = 0\n",
    "                self.tf[class_][word] += 1\n",
    "\n",
    "    def build_cf(self):\n",
    "        \"\"\"\n",
    "        Method to build CF for all words\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Performs\n",
    "        --------\n",
    "        1. For each word, calculates the number of classes in which it occurs\n",
    "        2. Stores the frequency in self.cf\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        for i in range(len(self.ls_text)):\n",
    "            current_class = self.ls_classes[i]\n",
    "            for word in self.ls_text[i]:\n",
    "                if word not in self.cf:\n",
    "                    self.cf[word] = set()\n",
    "                self.cf[word].add(current_class)\n",
    "        # replce set with length of set\n",
    "        for word in self.cf:\n",
    "            self.cf[word] = len(self.cf[word])\n",
    "    \n",
    "    def build_icf(self):\n",
    "        \"\"\"\n",
    "        Method to build ICF for all words\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Performs\n",
    "        --------\n",
    "        1. For each word, calculates the inverse class frequency\n",
    "        2. Stores the frequency in self.icf\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        for word in self.cf:\n",
    "            self.icf[word] = np.log10(len(self.classes)/self.cf[word])\n",
    "\n",
    "    def build_tf_icf(self):\n",
    "        \"\"\"\n",
    "        Method to build TF_ICF for all words\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Performs\n",
    "        --------\n",
    "        1. For each word, calculates the TF_ICF\n",
    "        2. Stores the frequency in self.tf_icf\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        for class_ in self.classes:\n",
    "            self.tf_icf[class_] = {}\n",
    "            for word in self.tf[class_]:\n",
    "                self.tf_icf[class_][word] = self.tf[class_][word] * self.icf[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(ls_text_train, ls_classes_train, ls_text_test, ls_classes_test, tf_icf, unique_classes, feature_set):\n",
    "    \"\"\"\n",
    "    Function to featurize the dataset using TF_ICF\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ls_text_train : list\n",
    "        list of training text\n",
    "\n",
    "    ls_classes_train : list\n",
    "        list of training classes\n",
    "\n",
    "    ls_text_test : list\n",
    "        list of test text\n",
    "\n",
    "    ls_classes_test : list\n",
    "        list of test classes\n",
    "\n",
    "    tf_icf : TF_ICF\n",
    "        TF_ICF object\n",
    "\n",
    "    unique_classes : list\n",
    "        list of unique classes\n",
    "\n",
    "    feature_set : list\n",
    "        list of features to be used\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_X : list\n",
    "        list of training features for each text\n",
    "\n",
    "    train_Y : list\n",
    "        list of training classes for each text\n",
    "\n",
    "    test_X : list\n",
    "        list of test features for each text\n",
    "\n",
    "    test_Y : list\n",
    "        list of test classes for each text\n",
    "    \"\"\"\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    test_X = []\n",
    "    test_Y = []\n",
    "    for mega_class in unique_classes:\n",
    "        # filter the dataset for the mega class\n",
    "        mega_class_ls_text_train = [ls_text_train[i] for i in range(len(ls_text_train)) if ls_classes_train[i] == mega_class]\n",
    "        mega_class_ls_classes_train = [ls_classes_train[i] for i in range(len(ls_classes_train)) if ls_classes_train[i] == mega_class]\n",
    "        mega_class_ls_text_test = [ls_text_test[i] for i in range(len(ls_text_test)) if ls_classes_test[i] == mega_class]\n",
    "        mega_class_ls_classes_test = [ls_classes_test[i] for i in range(len(ls_classes_test)) if ls_classes_test[i] == mega_class]\n",
    "        # featurize the dataset\n",
    "        for i in range(len(mega_class_ls_text_train)):\n",
    "            doc = []\n",
    "            for word in feature_set:\n",
    "                if word in mega_class_ls_text_train[i]:\n",
    "                    doc.append(mega_class_ls_text_train[i].count(word))\n",
    "                else:\n",
    "                    doc.append(0)\n",
    "            train_X.append(doc)\n",
    "            train_Y.append(mega_class_ls_classes_train[i])\n",
    "        for i in range(len(mega_class_ls_text_test)):\n",
    "            doc = []\n",
    "            for word in feature_set:\n",
    "                if word in mega_class_ls_text_test[i]:\n",
    "                    doc.append(mega_class_ls_text_test[i].count(word))\n",
    "                else:\n",
    "                    doc.append(0)\n",
    "            test_X.append(doc)\n",
    "            test_Y.append(mega_class_ls_classes_test[i])\n",
    "            \n",
    "    return train_X, train_Y, test_X, test_Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, split=0.7):\n",
    "    # Shuffling the data\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    # Splitting the data into train and test\n",
    "    train = df[:int(split*len(df))]\n",
    "    test = df[int(split*len(df)):]\n",
    "    # reset the index\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, split=0.7)\n",
    "\n",
    "tf_icf = TF_ICF(df_train['Category'].unique(), df_train['Text'], df_train['Category'])\n",
    "\n",
    "# Computes all the unique words in the dataset\n",
    "feature_set = []\n",
    "unique_classes = df_train['Category'].unique()\n",
    "for mega_class in unique_classes:\n",
    "    sub_tf_icf = tf_icf.tf_icf[mega_class]\n",
    "    for word, tficf in sub_tf_icf.items():\n",
    "        if word not in feature_set:\n",
    "            feature_set.append(word)\n",
    "feature_set = list(set(feature_set))\n",
    "\n",
    "train_X, train_Y, test_X, test_Y = featurize(df_train['Text'], df_train['Category'], df_test['Text'], df_test['Category'], tf_icf, unique_classes, feature_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        A Naive Bayes classifier with Laplace smoothing.\n",
    "        \"\"\"\n",
    "        self.num_features = None\n",
    "        self.class_labels = None\n",
    "        self.prior_prob = None\n",
    "        self.conditional_prob = None\n",
    "        self.cumulative_class_freq_stats = None\n",
    "        self.alpha = 1\n",
    "    \n",
    "    def train(self, train_x, train_y):\n",
    "        \"\"\"\n",
    "        Function to train the Naive Bayes classifier\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_x : list\n",
    "            list of training features for each text\n",
    "\n",
    "        train_y : list\n",
    "            list of training classes for each text\n",
    "        \"\"\"\n",
    "        total_class_samples = len(train_x)\n",
    "        self.num_features = len(train_x[0])\n",
    "        self.class_labels = list(set(train_y)) \n",
    "        self.prior_prob = {} \n",
    "        self.conditional_prob = {label:{} for label in self.class_labels}\n",
    "        self.cumulative_class_freq_stats = {label:{feat:0 for feat in range(self.num_features)} for label in self.class_labels} \n",
    "        class_wise_count = dict(Counter(train_y)) \n",
    "        for label in self.class_labels: \n",
    "            self.prior_prob[label] = float(class_wise_count[label]) / float(total_class_samples)\n",
    "        for i in range(total_class_samples): \n",
    "            sample_label = train_y[i]\n",
    "            for j in range(self.num_features):\n",
    "                self.cumulative_class_freq_stats[sample_label][j] += train_x[i][j]\n",
    "        \n",
    "        for label in self.class_labels:\n",
    "            for feature in range(self.num_features):\n",
    "                self.conditional_prob[label][feature] = float(self.cumulative_class_freq_stats[label][feature] + self.alpha ) / float(sum(self.cumulative_class_freq_stats[label].values()) + (self.num_features*self.alpha)) #conditional probab of a feature(word) wrt a class is the ratio of number of occurences of that word in that particular class divided by the sum of frequencies of all features(words) wrt that class.\n",
    "        \n",
    "        return self.prior_prob, self.conditional_prob\n",
    "\n",
    "    def predict(self, test_x):\n",
    "        predictions = []\n",
    "        for sample in test_x: \n",
    "            posterior_probs = {} \n",
    "            for label in self.class_labels: \n",
    "                probab = math.log10(self.prior_prob[label])  \n",
    "                for feature in range(len(sample)): \n",
    "                    if(sample[feature] != 0): \n",
    "                        probab += (math.log10(self.conditional_prob[label][feature]) * sample[feature]) \n",
    "                posterior_probs[label] = probab\n",
    "            pred_label = max(posterior_probs, key=posterior_probs.get) \n",
    "            predictions.append(pred_label)\n",
    "        return predictions\n",
    "\n",
    "def calculate_metrics(true_y, pred_y):\n",
    "    mapping = {0: 'Business', 1: 'Entertainment', 2: 'Politics', 3: 'Sport', 4: 'Tech'}\n",
    "    inverse_mapping = {'Business': 0, 'Entertainment': 1, 'Politics': 2, 'Sport': 3, 'Tech': 4}\n",
    "    # lower keys of mapping and inverse_mapping\n",
    "    mapping = {k: v.lower() for k, v in mapping.items()}\n",
    "    inverse_mapping = {k.lower(): v for k, v in inverse_mapping.items()}\n",
    "    \n",
    "    # Convert string labels to numeric labels\n",
    "    true_y = [inverse_mapping[y] for y in true_y]\n",
    "    pred_y = [inverse_mapping[y] for y in pred_y]\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    num_classes = len(mapping)\n",
    "    confusion_matrix = [[0 for _ in range(num_classes)] for _ in range(num_classes)]\n",
    "    for true_label, pred_label in zip(true_y, pred_y):\n",
    "        confusion_matrix[true_label][pred_label] += 1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = sum([confusion_matrix[i][i] for i in range(num_classes)])\n",
    "    total = sum([sum(confusion_matrix[i]) for i in range(num_classes)])\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # Calculate precision, recall, and f1-score for each class\n",
    "    metrics = {}\n",
    "    for i in range(num_classes):\n",
    "        tp = confusion_matrix[i][i]\n",
    "        fp = sum([confusion_matrix[j][i] for j in range(num_classes) if j != i])\n",
    "        fn = sum([confusion_matrix[i][j] for j in range(num_classes) if j != i])\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        metrics[mapping[i]] = {'precision': precision, 'recall': recall, 'f1-score': f1_score}\n",
    "    \n",
    "    # Macro-averaged precision, recall, and f1-score\n",
    "\n",
    "    macro_precision = sum([metrics[mapping[i]]['precision'] for i in range(num_classes)]) / num_classes\n",
    "    macro_recall = sum([metrics[mapping[i]]['recall'] for i in range(num_classes)]) / num_classes\n",
    "    macro_f1_score = sum([metrics[mapping[i]]['f1-score'] for i in range(num_classes)]) / num_classes\n",
    "    macro_metrics = {'precision': macro_precision, 'recall': macro_recall, 'f1-score': macro_f1_score}\n",
    "\n",
    "    return accuracy, metrics, macro_metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  1\n",
      "Accuracy:  0.9753914988814317\n",
      "Macro-averaged metrics: \n",
      "precision 0.9739492301290055\n",
      "recall 0.974875845074872\n",
      "f1-score 0.9738436157123648\n",
      "business\n",
      "precision 1.0\n",
      "recall 0.9387755102040817\n",
      "f1-score 0.968421052631579\n",
      "\n",
      "entertainment\n",
      "precision 1.0\n",
      "recall 0.9605263157894737\n",
      "f1-score 0.9798657718120806\n",
      "\n",
      "politics\n",
      "precision 0.9259259259259259\n",
      "recall 0.9868421052631579\n",
      "f1-score 0.9554140127388535\n",
      "\n",
      "sport\n",
      "precision 1.0\n",
      "recall 1.0\n",
      "f1-score 1.0\n",
      "\n",
      "tech\n",
      "precision 0.9438202247191011\n",
      "recall 0.9882352941176471\n",
      "f1-score 0.9655172413793103\n",
      "\n",
      "Alpha:  0.1\n",
      "Accuracy:  0.9731543624161074\n",
      "Macro-averaged metrics: \n",
      "precision 0.9709807828271897\n",
      "recall 0.9725563909774436\n",
      "f1-score 0.9713948330594144\n",
      "business\n",
      "precision 0.9891304347826086\n",
      "recall 0.9285714285714286\n",
      "f1-score 0.9578947368421052\n",
      "\n",
      "entertainment\n",
      "precision 0.9866666666666667\n",
      "recall 0.9736842105263158\n",
      "f1-score 0.9801324503311258\n",
      "\n",
      "politics\n",
      "precision 0.9240506329113924\n",
      "recall 0.9605263157894737\n",
      "f1-score 0.9419354838709677\n",
      "\n",
      "sport\n",
      "precision 1.0\n",
      "recall 1.0\n",
      "f1-score 1.0\n",
      "\n",
      "tech\n",
      "precision 0.9550561797752809\n",
      "recall 1.0\n",
      "f1-score 0.9770114942528736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.train(train_X, train_Y)\n",
    "pred_y = nb.predict(test_X)\n",
    "accuracy, metrics, macro_metrics = calculate_metrics(test_Y, pred_y)\n",
    "\n",
    "with open(f\"Saves/results_main.txt\", \"w\") as f:\n",
    "    f.write(\"Accuracy: \" + str(accuracy) + \"\\n\")      \n",
    "    for label in macro_metrics:         \n",
    "        f.write(label + \" \" + str(macro_metrics[label]) + \"\\n\")\n",
    "    for label in metrics:\n",
    "        f.write(label + \"\\n\")\n",
    "        for metric in metrics[label]:\n",
    "            f.write(metric + \" \" + str(metrics[label][metric]) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"Alpha: \", nb.alpha)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Macro-averaged metrics: \")\n",
    "for metric in macro_metrics:\n",
    "    print(metric, macro_metrics[metric])\n",
    "for label in metrics:\n",
    "    print(label)\n",
    "    for metric in metrics[label]:\n",
    "        print(metric, metrics[label][metric])\n",
    "    print()\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.alpha = 0.1\n",
    "nb.train(train_X, train_Y)\n",
    "pred_y = nb.predict(test_X)\n",
    "accuracy, metrics, macro_metrics = calculate_metrics(test_Y, pred_y)\n",
    "\n",
    "with open(f\"Saves/results_main_with_alpha_1e-1.txt\", \"w\") as f:\n",
    "    f.write(\"Accuracy: \" + str(accuracy) + \"\\n\")      \n",
    "    for label in macro_metrics:         \n",
    "        f.write(label + \" \" + str(macro_metrics[label]) + \"\\n\")\n",
    "\n",
    "    for label in metrics:\n",
    "        f.write(label + \"\\n\")\n",
    "        for metric in metrics[label]:\n",
    "            f.write(metric + \" \" + str(metrics[label][metric]) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "print(\"Alpha: \", nb.alpha)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Macro-averaged metrics: \")\n",
    "for label in macro_metrics:\n",
    "    print(label, macro_metrics[label])\n",
    "for label in metrics:\n",
    "    print(label)\n",
    "    for metric in metrics[label]:\n",
    "        print(metric, metrics[label][metric])\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Improving the Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Different preprocessing techniques and parameters to improve the performance of the classifier\n",
    "\n",
    "A - Stemming instead of lemmatization\n",
    "\n",
    "B - Not removing stopwords\n",
    "\n",
    "C - 60-40 split instead of 70-30\n",
    "\n",
    "D - 80-20 split instead of 70-30\n",
    "\n",
    "E - 90-10 split instead of 70-30\n",
    "\n",
    "F - 50-50 split instead of 70-30"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9798657718120806\n",
      "Macro-averaged metrics: \n",
      "precision 0.9794836364685349\n",
      "recall 0.9801349260786811\n",
      "f1-score 0.9796371215328188\n",
      "business\n",
      "precision 0.9789473684210527\n",
      "recall 0.9489795918367347\n",
      "f1-score 0.9637305699481866\n",
      "\n",
      "entertainment\n",
      "precision 1.0\n",
      "recall 0.9746835443037974\n",
      "f1-score 0.9871794871794872\n",
      "\n",
      "politics\n",
      "precision 0.9550561797752809\n",
      "recall 0.9770114942528736\n",
      "f1-score 0.9659090909090908\n",
      "\n",
      "sport\n",
      "precision 1.0\n",
      "recall 1.0\n",
      "f1-score 1.0\n",
      "\n",
      "tech\n",
      "precision 0.9634146341463414\n",
      "recall 1.0\n",
      "f1-score 0.9813664596273292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'Data\\BBC News Train.csv')\n",
    "df.drop('ArticleId', axis=1, inplace=True)\n",
    "df['Text'] = df['Text'].apply(remove_punctuation)\n",
    "df['Text'] = df['Text'].apply(lower_case)\n",
    "df['Text'] = df['Text'].apply(remove_stopwords)\n",
    "df['Text'] = df['Text'].apply(tokenize)\n",
    "df['Text'] = df['Text'].apply(stem)\n",
    "\n",
    "df_train, df_test = train_test_split(df, split=0.7)\n",
    "\n",
    "unique_classes = df_train['Category'].unique()\n",
    "\n",
    "tf_icf = TF_ICF(unique_classes, df_train['Text'], df_train['Category'])\n",
    "\n",
    "feature_set = []\n",
    "unique_classes = df_train['Category'].unique()\n",
    "for mega_class in unique_classes:\n",
    "    sub_tf_icf = tf_icf.tf_icf[mega_class]\n",
    "    for word, tficf in sub_tf_icf.items():\n",
    "        if word not in feature_set:\n",
    "            feature_set.append(word)\n",
    "feature_set = list(set(feature_set))\n",
    "\n",
    "train_X, train_Y, test_X, test_Y = featurize(df_train['Text'], df_train['Category'], df_test['Text'], df_test['Category'], tf_icf, unique_classes, feature_set)\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.train(train_X, train_Y)\n",
    "pred_y = nb.predict(test_X)\n",
    "accuracy, metrics, macro_metrics = calculate_metrics(test_Y, pred_y)\n",
    "\n",
    "with open(f\"Saves/results_stemming.txt\", \"w\") as f:\n",
    "    f.write(\"Accuracy: \" + str(accuracy) + \"\\n\")      \n",
    "    for label in macro_metrics:         \n",
    "        f.write(label + \" \" + str(macro_metrics[label]) + \"\\n\")\n",
    "    for label in metrics:\n",
    "        f.write(label + \"\\n\")\n",
    "        for metric in metrics[label]:\n",
    "            f.write(metric + \" \" + str(metrics[label][metric]) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Macro-averaged metrics: \")\n",
    "for label in macro_metrics:\n",
    "    print(label, macro_metrics[label])\n",
    "for label in metrics:\n",
    "    print(label)\n",
    "    for metric in metrics[label]:\n",
    "        print(metric, metrics[label][metric])\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9642058165548099\n",
      "Macro-averaged metrics: \n",
      "precision 0.9635892377885131\n",
      "recall 0.9630680380868373\n",
      "f1-score 0.9627551179176397\n",
      "business\n",
      "precision 0.98\n",
      "recall 0.98\n",
      "f1-score 0.98\n",
      "\n",
      "entertainment\n",
      "precision 1.0\n",
      "recall 0.9176470588235294\n",
      "f1-score 0.9570552147239264\n",
      "\n",
      "politics\n",
      "precision 0.9135802469135802\n",
      "recall 0.961038961038961\n",
      "f1-score 0.9367088607594937\n",
      "\n",
      "sport\n",
      "precision 0.9895833333333334\n",
      "recall 0.979381443298969\n",
      "f1-score 0.9844559585492227\n",
      "\n",
      "tech\n",
      "precision 0.9347826086956522\n",
      "recall 0.9772727272727273\n",
      "f1-score 0.9555555555555557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'Data\\BBC News Train.csv')\n",
    "df.drop('ArticleId', axis=1, inplace=True)\n",
    "df['Text'] = df['Text'].apply(remove_punctuation)\n",
    "df['Text'] = df['Text'].apply(lower_case)\n",
    "df['Text'] = df['Text'].apply(tokenize)\n",
    "df['Text'] = df['Text'].apply(lemmatize)\n",
    "\n",
    "df_train, df_test = train_test_split(df, split=0.7)\n",
    "\n",
    "feature_set = []\n",
    "unique_classes = df_train['Category'].unique()\n",
    "for mega_class in unique_classes:\n",
    "    sub_tf_icf = tf_icf.tf_icf[mega_class]\n",
    "    for word, tficf in sub_tf_icf.items():\n",
    "        if word not in feature_set:\n",
    "            feature_set.append(word)\n",
    "feature_set = list(set(feature_set))\n",
    "\n",
    "train_X, train_Y, test_X, test_Y = featurize(df_train['Text'], df_train['Category'], df_test['Text'], df_test['Category'], tf_icf, unique_classes, feature_set)\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.train(train_X, train_Y)\n",
    "pred_y = nb.predict(test_X)\n",
    "accuracy, metrics, macro_metrics = calculate_metrics(test_Y, pred_y)\n",
    "\n",
    "with open(f\"Saves/results_no_stopwords_removed.txt\", \"w\") as f:\n",
    "    f.write(\"Accuracy: \" + str(accuracy) + \"\\n\")      \n",
    "    for label in macro_metrics:         \n",
    "        f.write(label + \" \" + str(macro_metrics[label]) + \"\\n\")\n",
    "    for label in metrics:\n",
    "        f.write(label + \"\\n\")\n",
    "        for metric in metrics[label]:\n",
    "            f.write(metric + \" \" + str(metrics[label][metric]) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Macro-averaged metrics: \")\n",
    "for label in macro_metrics:\n",
    "    print(label, macro_metrics[label])\n",
    "for label in metrics:\n",
    "    print(label)\n",
    "    for metric in metrics[label]:\n",
    "        print(metric, metrics[label][metric])\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C, D, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:  0.6\n",
      "Accuracy:  0.947986577181208\n",
      "Macro-averaged metrics: \n",
      "precision 0.9456123815822272\n",
      "recall 0.9482966126984025\n",
      "f1-score 0.9456552331512895\n",
      "business\n",
      "precision 0.965034965034965\n",
      "recall 0.9139072847682119\n",
      "f1-score 0.9387755102040817\n",
      "\n",
      "entertainment\n",
      "precision 1.0\n",
      "recall 0.9047619047619048\n",
      "f1-score 0.9500000000000001\n",
      "\n",
      "politics\n",
      "precision 0.9016393442622951\n",
      "recall 0.9565217391304348\n",
      "f1-score 0.9282700421940927\n",
      "\n",
      "sport\n",
      "precision 0.9927007299270073\n",
      "recall 1.0\n",
      "f1-score 0.9963369963369962\n",
      "\n",
      "tech\n",
      "precision 0.8686868686868687\n",
      "recall 0.9662921348314607\n",
      "f1-score 0.9148936170212767\n",
      "\n",
      "Split:  0.8\n",
      "Accuracy:  0.9630872483221476\n",
      "Macro-averaged metrics: \n",
      "precision 0.962337363564326\n",
      "recall 0.9635230643023917\n",
      "f1-score 0.9618617454450613\n",
      "business\n",
      "precision 1.0\n",
      "recall 0.9130434782608695\n",
      "f1-score 0.9545454545454545\n",
      "\n",
      "entertainment\n",
      "precision 1.0\n",
      "recall 0.9423076923076923\n",
      "f1-score 0.9702970297029703\n",
      "\n",
      "politics\n",
      "precision 0.9310344827586207\n",
      "recall 1.0\n",
      "f1-score 0.9642857142857143\n",
      "\n",
      "sport\n",
      "precision 0.9859154929577465\n",
      "recall 1.0\n",
      "f1-score 0.9929078014184397\n",
      "\n",
      "tech\n",
      "precision 0.8947368421052632\n",
      "recall 0.9622641509433962\n",
      "f1-score 0.9272727272727272\n",
      "\n",
      "Split:  0.9\n",
      "Accuracy:  0.959731543624161\n",
      "Macro-averaged metrics: \n",
      "precision 0.953781512605042\n",
      "recall 0.9608067226890757\n",
      "f1-score 0.9558216932257853\n",
      "business\n",
      "precision 0.9705882352941176\n",
      "recall 0.9428571428571428\n",
      "f1-score 0.9565217391304348\n",
      "\n",
      "entertainment\n",
      "precision 1.0\n",
      "recall 0.92\n",
      "f1-score 0.9583333333333334\n",
      "\n",
      "politics\n",
      "precision 0.8571428571428571\n",
      "recall 1.0\n",
      "f1-score 0.923076923076923\n",
      "\n",
      "sport\n",
      "precision 1.0\n",
      "recall 1.0\n",
      "f1-score 1.0\n",
      "\n",
      "tech\n",
      "precision 0.9411764705882353\n",
      "recall 0.9411764705882353\n",
      "f1-score 0.9411764705882353\n",
      "\n",
      "Split:  0.5\n",
      "Accuracy:  0.9624161073825503\n",
      "Macro-averaged metrics: \n",
      "precision 0.9610456250200092\n",
      "recall 0.9617318570853218\n",
      "f1-score 0.960955571549917\n",
      "business\n",
      "precision 0.9748427672955975\n",
      "recall 0.9451219512195121\n",
      "f1-score 0.9597523219814241\n",
      "\n",
      "entertainment\n",
      "precision 0.9849624060150376\n",
      "recall 0.9290780141843972\n",
      "f1-score 0.9562043795620437\n",
      "\n",
      "politics\n",
      "precision 0.9455782312925171\n",
      "recall 0.9788732394366197\n",
      "f1-score 0.9619377162629758\n",
      "\n",
      "sport\n",
      "precision 0.9940476190476191\n",
      "recall 0.9940476190476191\n",
      "f1-score 0.9940476190476191\n",
      "\n",
      "tech\n",
      "precision 0.9057971014492754\n",
      "recall 0.9615384615384616\n",
      "f1-score 0.9328358208955224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'Data\\BBC News Train.csv')\n",
    "df.drop('ArticleId', axis=1, inplace=True)\n",
    "df['Text'] = df['Text'].apply(remove_punctuation)\n",
    "df['Text'] = df['Text'].apply(lower_case)\n",
    "df['Text'] = df['Text'].apply(remove_stopwords)\n",
    "df['Text'] = df['Text'].apply(tokenize)\n",
    "df['Text'] = df['Text'].apply(lemmatize)\n",
    "\n",
    "splits = [0.6, 0.8, 0.9, 0.5]\n",
    "\n",
    "for split in splits:\n",
    "    print(\"Split: \", split)\n",
    "    df_train, df_test = train_test_split(df, split=split)\n",
    "\n",
    "    feature_set = []\n",
    "    unique_classes = df_train['Category'].unique()\n",
    "    for mega_class in unique_classes:\n",
    "        sub_tf_icf = tf_icf.tf_icf[mega_class]\n",
    "        for word, tficf in sub_tf_icf.items():\n",
    "            if word not in feature_set:\n",
    "                feature_set.append(word)\n",
    "    feature_set = list(set(feature_set))\n",
    "\n",
    "    train_X, train_Y, test_X, test_Y = featurize(df_train['Text'], df_train['Category'], df_test['Text'], df_test['Category'], tf_icf, unique_classes, feature_set)\n",
    "\n",
    "    nb = NaiveBayes()\n",
    "    nb.train(train_X, train_Y)\n",
    "    pred_y = nb.predict(test_X)\n",
    "    accuracy, metrics, macro_metrics = calculate_metrics(test_Y, pred_y)\n",
    "    # Write to file\n",
    "    with open(f\"Saves/results_{split}.txt\", \"w\") as f:\n",
    "        f.write(\"Accuracy: \" + str(accuracy) + \"\\n\")      \n",
    "        for label in macro_metrics:         \n",
    "            f.write(label + \" \" + str(macro_metrics[label]) + \"\\n\")\n",
    "        for label in metrics:\n",
    "            f.write(label + \"\\n\")\n",
    "            for metric in metrics[label]:\n",
    "                f.write(metric + \" \" + str(metrics[label][metric]) + \"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Macro-averaged metrics: \")\n",
    "    for label in macro_metrics:\n",
    "        print(label, macro_metrics[label])\n",
    "    for label in metrics:\n",
    "        print(label)\n",
    "        for metric in metrics[label]:\n",
    "            print(metric, metrics[label][metric])\n",
    "        print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Different types of features such as n-grams or TF-IDF weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gram_range=(1, 1) for unigram\n",
      "Accuracy:  0.9642058165548099\n",
      "Macro-averaged metrics: \n",
      "precision 0.9646629914707718\n",
      "recall 0.9662462038009025\n",
      "f1-score 0.964441890189276\n",
      "business\n",
      "precision 0.9789473684210527\n",
      "recall 0.8942307692307693\n",
      "f1-score 0.9346733668341709\n",
      "\n",
      "entertainment\n",
      "precision 1.0\n",
      "recall 0.958904109589041\n",
      "f1-score 0.9790209790209791\n",
      "\n",
      "politics\n",
      "precision 0.9204545454545454\n",
      "recall 0.9878048780487805\n",
      "f1-score 0.9529411764705882\n",
      "\n",
      "sport\n",
      "precision 1.0\n",
      "recall 0.9902912621359223\n",
      "f1-score 0.9951219512195122\n",
      "\n",
      "tech\n",
      "precision 0.9239130434782609\n",
      "recall 1.0\n",
      "f1-score 0.96045197740113\n",
      "\n",
      "n_gram_range=(1, 2) for unigrams and bigrams\n",
      "Accuracy:  0.9642058165548099\n",
      "Macro-averaged metrics: \n",
      "precision 0.9646629914707718\n",
      "recall 0.9662462038009025\n",
      "f1-score 0.964441890189276\n",
      "business\n",
      "precision 0.9789473684210527\n",
      "recall 0.8942307692307693\n",
      "f1-score 0.9346733668341709\n",
      "\n",
      "entertainment\n",
      "precision 1.0\n",
      "recall 0.958904109589041\n",
      "f1-score 0.9790209790209791\n",
      "\n",
      "politics\n",
      "precision 0.9204545454545454\n",
      "recall 0.9878048780487805\n",
      "f1-score 0.9529411764705882\n",
      "\n",
      "sport\n",
      "precision 1.0\n",
      "recall 0.9902912621359223\n",
      "f1-score 0.9951219512195122\n",
      "\n",
      "tech\n",
      "precision 0.9239130434782609\n",
      "recall 1.0\n",
      "f1-score 0.96045197740113\n",
      "\n",
      "n_gram_range=(2, 2) for bigrams\n",
      "Accuracy:  0.9642058165548099\n",
      "business\n",
      "precision 0.9789473684210527\n",
      "recall 0.8942307692307693\n",
      "f1-score 0.9346733668341709\n",
      "\n",
      "entertainment\n",
      "precision 1.0\n",
      "recall 0.958904109589041\n",
      "f1-score 0.9790209790209791\n",
      "\n",
      "politics\n",
      "precision 0.9204545454545454\n",
      "recall 0.9878048780487805\n",
      "f1-score 0.9529411764705882\n",
      "\n",
      "sport\n",
      "precision 1.0\n",
      "recall 0.9902912621359223\n",
      "f1-score 0.9951219512195122\n",
      "\n",
      "tech\n",
      "precision 0.9239130434782609\n",
      "recall 1.0\n",
      "f1-score 0.96045197740113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "df = pd.read_csv(r'Data\\BBC News Train.csv')\n",
    "df.drop('ArticleId', axis=1, inplace=True)\n",
    "df['Text'] = df['Text'].apply(remove_punctuation)\n",
    "df['Text'] = df['Text'].apply(lower_case)\n",
    "df['Text'] = df['Text'].apply(remove_stopwords)\n",
    "df['Text'] = df['Text'].apply(tokenize)\n",
    "df['Text'] = df['Text'].apply(lemmatize)\n",
    "df['Text'] = df['Text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "df_train, df_test = train_test_split(df, split=0.7)\n",
    "\n",
    "# n_gram_range=(1, 1) for unigram\n",
    "print(\"n_gram_range=(1, 1) for unigram\")\n",
    "vectorizer.fit(df_train['Text'])\n",
    "\n",
    "train_X = vectorizer.transform(df_train['Text']).todense().tolist()\n",
    "train_Y = df_train['Category']\n",
    "\n",
    "test_X = vectorizer.transform(df_test['Text']).todense().tolist()\n",
    "test_Y = df_test['Category']\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.train(train_X, train_Y)\n",
    "pred_y = nb.predict(test_X)\n",
    "accuracy, metrics, macro_metrics = calculate_metrics(test_Y, pred_y)\n",
    "\n",
    "with open(f\"Saves/results_unigram.txt\", \"w\") as f:\n",
    "    f.write(\"Accuracy: \" + str(accuracy) + \"\\n\")      \n",
    "    for label in macro_metrics:         \n",
    "        f.write(label + \" \" + str(macro_metrics[label]) + \"\\n\")\n",
    "    for label in metrics:\n",
    "        f.write(label + \"\\n\")\n",
    "        for metric in metrics[label]:\n",
    "            f.write(metric + \" \" + str(metrics[label][metric]) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Macro-averaged metrics: \")\n",
    "for label in macro_metrics:\n",
    "    print(label, macro_metrics[label])\n",
    "for label in metrics:\n",
    "    print(label)\n",
    "    for metric in metrics[label]:\n",
    "        print(metric, metrics[label][metric])\n",
    "    print()\n",
    "\n",
    "# n_gram_range=(1, 2) for unigrams and bigrams\n",
    "print(\"n_gram_range=(1, 2) for unigrams and bigrams\")\n",
    "vectorizer.fit(df_train['Text'])\n",
    "\n",
    "train_X = vectorizer.transform(df_train['Text']).todense().tolist()\n",
    "train_Y = df_train['Category']\n",
    "\n",
    "test_X = vectorizer.transform(df_test['Text']).todense().tolist()\n",
    "test_Y = df_test['Category']\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.train(train_X, train_Y)\n",
    "pred_y = nb.predict(test_X)\n",
    "accuracy, metrics, macro_metrics = calculate_metrics(test_Y, pred_y)\n",
    "\n",
    "with open(f\"Saves/results_unigram_and_bigram.txt\", \"w\") as f:\n",
    "    f.write(\"Accuracy: \" + str(accuracy) + \"\\n\")      \n",
    "    for label in macro_metrics:         \n",
    "        f.write(label + \" \" + str(macro_metrics[label]) + \"\\n\")\n",
    "    for label in metrics:\n",
    "        f.write(label + \"\\n\")\n",
    "        for metric in metrics[label]:\n",
    "            f.write(metric + \" \" + str(metrics[label][metric]) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Macro-averaged metrics: \")\n",
    "for label in macro_metrics:\n",
    "    print(label, macro_metrics[label])\n",
    "for label in metrics:\n",
    "    print(label)\n",
    "    for metric in metrics[label]:\n",
    "        print(metric, metrics[label][metric])\n",
    "    print()\n",
    "\n",
    "# n_gram_range=(2, 2) for bigrams\n",
    "print(\"n_gram_range=(2, 2) for bigrams\")\n",
    "vectorizer.fit(df_train['Text'])\n",
    "\n",
    "train_X = vectorizer.transform(df_train['Text']).todense().tolist()\n",
    "train_Y = df_train['Category']\n",
    "\n",
    "test_X = vectorizer.transform(df_test['Text']).todense().tolist()\n",
    "test_Y = df_test['Category']\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.train(train_X, train_Y)\n",
    "pred_y = nb.predict(test_X)\n",
    "accuracy, metrics, macro_metrics = calculate_metrics(test_Y, pred_y)\n",
    "\n",
    "with open(f\"Saves/results_bigram.txt\", \"w\") as f:\n",
    "    f.write(\"Accuracy: \" + str(accuracy) + \"\\n\")      \n",
    "    for label in macro_metrics:         \n",
    "        f.write(label + \" \" + str(macro_metrics[label]) + \"\\n\")\n",
    "    for label in metrics:\n",
    "        f.write(label + \"\\n\")\n",
    "        for metric in metrics[label]:\n",
    "            f.write(metric + \" \" + str(metrics[label][metric]) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "for label in metrics:\n",
    "    print(label)\n",
    "    for metric in metrics[label]:\n",
    "        print(metric, metrics[label][metric])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9642058165548099\n",
      "Macro-averaged metrics: \n",
      "precision 0.9664366083986338\n",
      "recall 0.9625157041769073\n",
      "f1-score 0.963786951933636\n",
      "business\n",
      "precision 0.9142857142857143\n",
      "recall 0.9795918367346939\n",
      "f1-score 0.9458128078817734\n",
      "\n",
      "entertainment\n",
      "precision 1.0\n",
      "recall 0.9325842696629213\n",
      "f1-score 0.9651162790697675\n",
      "\n",
      "politics\n",
      "precision 0.9873417721518988\n",
      "recall 0.9285714285714286\n",
      "f1-score 0.9570552147239264\n",
      "\n",
      "sport\n",
      "precision 0.9722222222222222\n",
      "recall 1.0\n",
      "f1-score 0.9859154929577464\n",
      "\n",
      "tech\n",
      "precision 0.9583333333333334\n",
      "recall 0.971830985915493\n",
      "f1-score 0.965034965034965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "df = pd.read_csv(r'Data\\BBC News Train.csv')\n",
    "df.drop('ArticleId', axis=1, inplace=True)\n",
    "df['Text'] = df['Text'].apply(remove_punctuation)\n",
    "df['Text'] = df['Text'].apply(lower_case) \n",
    "df['Text'] = df['Text'].apply(remove_stopwords)\n",
    "df['Text'] = df['Text'].apply(tokenize)\n",
    "df['Text'] = df['Text'].apply(lemmatize)\n",
    "df['Text'] = df['Text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "df_train, df_test = train_test_split(df, split=0.7)\n",
    "\n",
    "# tfidf\n",
    "vectorizer.fit(df_train['Text'])\n",
    "\n",
    "train_X = vectorizer.transform(df_train['Text']).todense().tolist()\n",
    "train_Y = df_train['Category']\n",
    "\n",
    "test_X = vectorizer.transform(df_test['Text']).todense().tolist()\n",
    "test_Y = df_test['Category']\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.train(train_X, train_Y)\n",
    "pred_y = nb.predict(test_X)\n",
    "accuracy, metrics, macro_metrics = calculate_metrics(test_Y, pred_y)\n",
    "with open(f\"Saves/results_tfidf.txt\", \"w\") as f:\n",
    "    f.write(\"Accuracy: \" + str(accuracy) + \"\\n\")      \n",
    "    for label in macro_metrics:         \n",
    "        f.write(label + \" \" + str(macro_metrics[label]) + \"\\n\")\n",
    "    for label in metrics:\n",
    "        f.write(label + \"\\n\")\n",
    "        for metric in metrics[label]:\n",
    "            f.write(metric + \" \" + str(metrics[label][metric]) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Macro-averaged metrics: \")\n",
    "for label in macro_metrics:\n",
    "    print(label, macro_metrics[label])\n",
    "for label in metrics:\n",
    "    print(label)\n",
    "    for metric in metrics[label]:\n",
    "        print(metric, metrics[label][metric])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
